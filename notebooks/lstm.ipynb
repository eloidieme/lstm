{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eloidieme/dev/python-projects/lstm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eloidieme/dev/python-projects/lstm/env/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107542"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_fname = \"./data/goblet_book.txt\"\n",
    "with open(book_fname, 'r') as book:\n",
    "    book_data = book.read()\n",
    "len(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, train_frac=0.8, val_frac=0.1):\n",
    "    train_end = int(len(text) * train_frac)\n",
    "    val_end = train_end + int(len(text) * val_frac)\n",
    "\n",
    "    train_data = text[:train_end]\n",
    "    val_data = text[train_end:val_end]\n",
    "    test_data = text[val_end:]\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = book_data.split()\n",
    "chars = [[*word] for word in word_list]\n",
    "max_len = max(len(word) for word in chars)\n",
    "for wordl in chars:\n",
    "    while len(wordl) < max_len:\n",
    "        wordl.append(' ')\n",
    "chars = np.array(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = list(np.unique(chars))\n",
    "unique_chars.append('\\n')\n",
    "unique_chars.append('\\t')\n",
    "K = len(unique_chars)  # dimensionality of the input and output vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {}\n",
    "ind_to_char = {}\n",
    "for idx, char in enumerate(unique_chars):\n",
    "    char_to_ind[char] = idx\n",
    "    ind_to_char[idx] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100  # dimensionality of the hidden state\n",
    "eta = 0.1  # learning rate\n",
    "seq_length = 25  # length of input sequences used during training\n",
    "epsilon = 1e-8  # for AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = 0.01\n",
    "Wf = torch.normal(0.0, sig, (m, m), dtype=torch.double, requires_grad=True)\n",
    "Wi = torch.normal(0.0, sig, (m, m), dtype=torch.double, requires_grad=True)\n",
    "Wo = torch.normal(0.0, sig, (m, m), dtype=torch.double, requires_grad=True)\n",
    "Wc = torch.normal(0.0, sig, (m, m), dtype=torch.double, requires_grad=True)\n",
    "Wlist = [Wf, Wi, Wo, Wc]\n",
    "Wall = torch.cat(Wlist, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uf = torch.normal(0.0, sig, (m, K), dtype=torch.double, requires_grad=True)\n",
    "Ui = torch.normal(0.0, sig, (m, K), dtype=torch.double, requires_grad=True)\n",
    "Uo = torch.normal(0.0, sig, (m, K), dtype=torch.double, requires_grad=True)\n",
    "Uc = torch.normal(0.0, sig, (m, K), dtype=torch.double, requires_grad=True)\n",
    "Ulist = [Uf, Ui, Uo, Uc]\n",
    "Uall = torch.cat(Ulist, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = torch.normal(0.0, sig, (K, m), dtype=torch.double, requires_grad=True)\n",
    "c = torch.zeros((K, 1), dtype=torch.double, requires_grad=True)\n",
    "FClist = [V, c]\n",
    "FC = torch.cat(FClist, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM = {\n",
    "    'Wall': Wall, \n",
    "    'Uall': Uall,\n",
    "    'FC': FC\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "E1 = torch.cat([torch.eye(m, dtype=torch.double), torch.zeros((m, m), dtype=torch.double), torch.zeros((m, m), dtype=torch.double), torch.zeros((m, m), dtype=torch.double)], dim=1)\n",
    "E2 = torch.cat([torch.zeros((m, m), dtype=torch.double), torch.eye(m, dtype=torch.double), torch.zeros((m, m), dtype=torch.double), torch.zeros((m, m), dtype=torch.double)], dim=1)\n",
    "E3 = torch.cat([torch.zeros((m, m), dtype=torch.double), torch.zeros((m, m), dtype=torch.double), torch.eye(m, dtype=torch.double), torch.zeros((m, m), dtype=torch.double)], dim=1)\n",
    "E4 = torch.cat([torch.zeros((m, m), dtype=torch.double), torch.zeros((m, m), dtype=torch.double), torch.zeros((m, m), dtype=torch.double), torch.eye(m, dtype=torch.double)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char(char):\n",
    "    oh = [0]*K\n",
    "    oh[char_to_ind[char]] = 1\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetize_seq(lstm, h0, c0, x0, n, T = 1):\n",
    "    t, ht, ct, xt = 0, h0.clone(), c0.clone(), x0.clone().reshape((K, 1))\n",
    "    indexes = []\n",
    "    while t < n:\n",
    "        at = torch.mm(lstm['Wall'], ht) + torch.mm(lstm['Uall'], xt)\n",
    "        ft = F.sigmoid(torch.mm(E1, at))\n",
    "        it = F.sigmoid(torch.mm(E2, at))\n",
    "        ot = F.sigmoid(torch.mm(E3, at))\n",
    "        ctilde = F.tanh(torch.mm(E4, at))\n",
    "        ct = ft * ct + it * ctilde\n",
    "        ht = ot * F.tanh(ct)\n",
    "        out = torch.mm(lstm['FC'][:, :-1], ht) + lstm['FC'][:, -1:]\n",
    "        pt = F.softmax(out/T, dim=0)\n",
    "        cp = torch.cumsum(pt, dim=0)\n",
    "        a = torch.rand(1)\n",
    "        ixs = torch.where(cp - a > 0)\n",
    "        ii = ixs[0][0].item()\n",
    "        indexes.append(ii)\n",
    "        xt = torch.zeros((K, 1), dtype=torch.double)\n",
    "        xt[ii, 0] = 1\n",
    "        t += 1\n",
    "    Y = []\n",
    "    for idx in indexes:\n",
    "        oh = [0]*K\n",
    "        oh[idx] = 1\n",
    "        Y.append(oh)\n",
    "    Y = torch.tensor(Y).t()\n",
    "    \n",
    "    s = ''\n",
    "    for i in range(Y.shape[1]):\n",
    "        idx = torch.where(Y[:, i] == 1)[0].item()\n",
    "        s += ind_to_char[idx]\n",
    "    \n",
    "    return Y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_string(chars):\n",
    "    M = []\n",
    "    for i in range(len(chars)):\n",
    "        M.append(encode_char(chars[i]))\n",
    "    M = torch.tensor(M, dtype=torch.double).t()\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(lstm, X, hprev, cprev):\n",
    "    ht = hprev.clone()\n",
    "    ct = cprev.clone()\n",
    "    P = torch.zeros((K, seq_length), dtype=torch.double)\n",
    "    for i in range(seq_length):\n",
    "        xt = X[:, i].reshape((K, 1))\n",
    "        at = torch.mm(lstm['Wall'], ht) + torch.mm(lstm['Uall'], xt)\n",
    "        ft = F.sigmoid(torch.mm(E1, at))\n",
    "        it = F.sigmoid(torch.mm(E2, at))\n",
    "        ot = F.sigmoid(torch.mm(E3, at))\n",
    "        ctilde = F.tanh(torch.mm(E4, at))\n",
    "        ct = ft * ct + it * ctilde\n",
    "        ht = ot * F.tanh(ct)\n",
    "        out = torch.mm(lstm['FC'][:, :-1], ht) + lstm['FC'][:, -1:]\n",
    "        pt = F.softmax(out, dim=0)\n",
    "\n",
    "        P[:, i] = pt.squeeze()\n",
    "\n",
    "    return P, ht, ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, P):\n",
    "    log_probs = torch.log(P)\n",
    "    cross_entropy = -torch.sum(Y * log_probs)\n",
    "    loss = cross_entropy.item()\n",
    "    return cross_entropy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(rnn, val_data):\n",
    "    total_loss = 0\n",
    "    total_characters = 0\n",
    "    hprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "    for i in range(0, len(val_data) - seq_length, seq_length):\n",
    "        X_chars = val_data[i:i + seq_length]\n",
    "        Y_chars = val_data[i + 1:i + seq_length + 1]\n",
    "        X_val = encode_string(X_chars)\n",
    "        Y_val = encode_string(Y_chars)\n",
    "        _, _, P, hprev = forward(rnn, X_val, hprev)\n",
    "        loss = compute_loss(Y_val, P)\n",
    "        total_loss += loss * seq_length\n",
    "        total_characters += seq_length\n",
    "    average_loss = total_loss / total_characters\n",
    "    perplexity = torch.exp(torch.tensor(average_loss))\n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "\t * Smooth loss: 109.54975949941455\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Synthetized sequence: \n",
      "}EHü9/g'7fQCm(p;/UL';aüOInniqbbVI1YgyYJGB,R;bV0YpRü!:'kT?vgC-Ds(SoU_e0aa!j:cfdPS}M\"ureH(?:Q,?D7VMzü'7yh;vZgyI.9QrP}}2otwBzaB6TxT!CwrYRCGY,j:m0H•gyOS_üi7E^Z}^.pXue-B:ANNNWl}jKH7k6K_3wü^}6!;m/r:gr•nTEQC\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step: 1000\n",
      "\t * Smooth loss: 74.8556932532987\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m P_train, ht, ct \u001b[38;5;241m=\u001b[39m forward(LSTM, X_train, hprev, cprev)\n\u001b[1;32m     51\u001b[0m cross_entropy, loss \u001b[38;5;241m=\u001b[39m compute_loss(Y_train, P_train)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mcross_entropy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Ws):\n\u001b[1;32m     55\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(Wlist[idx]\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/dev/python-projects/lstm/env/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/python-projects/lstm/env/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "e, step, epoch = 0, 0, 0\n",
    "n_epochs = 2\n",
    "smooth_loss = 0\n",
    "seq_length = 25\n",
    "losses = []\n",
    "hprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "cprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "\n",
    "mWf = torch.zeros_like(Wf, dtype=torch.double)\n",
    "mWi = torch.zeros_like(Wi, dtype=torch.double)\n",
    "mWo = torch.zeros_like(Wo, dtype=torch.double)\n",
    "mWc = torch.zeros_like(Wc, dtype=torch.double)\n",
    "mUf = torch.zeros_like(Uf, dtype=torch.double)\n",
    "mUi = torch.zeros_like(Ui, dtype=torch.double)\n",
    "mUo = torch.zeros_like(Uo, dtype=torch.double)\n",
    "mUc = torch.zeros_like(Uc, dtype=torch.double)\n",
    "mV = torch.zeros_like(V, dtype=torch.double)\n",
    "mc = torch.zeros_like(c, dtype=torch.double)\n",
    "msW = {\n",
    "    'Wf': mWf, \n",
    "    'Wi': mWi, \n",
    "    'Wo': mWo, \n",
    "    'Wc': mWc, \n",
    "}\n",
    "msU = {\n",
    "    'Uf': mUf,\n",
    "    'Ui': mUi,\n",
    "    'Uo': mUo,\n",
    "    'Uc': mUc\n",
    "}\n",
    "msFC = {\n",
    "    'V': mV,\n",
    "    'c': mc\n",
    "}\n",
    "Ws = ['Wf', 'Wi', 'Wo', 'Wc']\n",
    "Us = ['Uf', 'Ui', 'Uo', 'Uc']\n",
    "FCs = ['V', 'c']\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    for p in Wlist + Ulist + FClist:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    X_chars = book_data[e:e+seq_length]\n",
    "    Y_chars = book_data[e+1:e+seq_length+1]\n",
    "    X_train = encode_string(X_chars)\n",
    "    Y_train = encode_string(Y_chars)\n",
    "\n",
    "    P_train, ht, ct = forward(LSTM, X_train, hprev, cprev)\n",
    "    cross_entropy, loss = compute_loss(Y_train, P_train)\n",
    "    cross_entropy.backward(retain_graph=True)\n",
    "\n",
    "    for idx, key in enumerate(Ws):\n",
    "        grad = torch.clamp(Wlist[idx].grad, -5, 5)\n",
    "        msW[key] += grad**2\n",
    "        LSTM['Wall'][100*idx:100*(idx+1)] -= (eta/torch.sqrt(msW[key] + epsilon))*grad\n",
    "\n",
    "    for idx, key in enumerate(Us):\n",
    "        grad = torch.clamp(Ulist[idx].grad, -5, 5)\n",
    "        msU[key] += grad**2\n",
    "        LSTM['Uall'][100*idx:100*(idx+1)] -= (eta/torch.sqrt(msU[key] + epsilon))*grad\n",
    "\n",
    "    for idx, key in enumerate(FCs):\n",
    "        grad = torch.clamp(FClist[idx].grad, -5, 5)\n",
    "        msFC[key] += grad**2\n",
    "        LSTM['FC'][:, -1*idx:-1*(1-idx)] -= (eta/torch.sqrt(msFC[key] + epsilon))*grad\n",
    "\n",
    "    if step == 0:\n",
    "        smooth_loss = loss\n",
    "    else:\n",
    "        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "\n",
    "    losses.append(smooth_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"\\t * Smooth loss: {smooth_loss}\")\n",
    "    if step % 5000 == 0:\n",
    "        _, s_syn = synthetize_seq(LSTM, hprev, cprev, X_train[:, 0], 200, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "        print(\"-\" * 100)\n",
    "    if step % 100000 == 0 and step > 0:\n",
    "        _, s_lsyn = synthetize_seq(LSTM, hprev, cprev, X_train[:, 0], 1000, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    step += 1\n",
    "    e += seq_length\n",
    "    if e > len(book_data) - seq_length:\n",
    "        e = 0\n",
    "        epoch += 1\n",
    "        hprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "        cprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "    else:\n",
    "        hprev = ht.detach()\n",
    "        cprev = ct.detach()\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(LSTM, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Smooth loss')\n",
    "plt.title(f'Training - eta: {eta} - seq_length: {seq_length} - m: {m} - n_epochs: {n_epochs}')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rnn_eminem.pickle', 'rb') as handle:\n",
    "    test_rnn = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " He on to his the cartar the his that coras een at he rolleconted whas has abuiss weer Harry Harrent had his creor scho budiut theel the inguped wish he had has youlr of sicinging in he rouse tham Sare whoy be all had wearly, had ine Wirine haire wark ther he had hes had sore, a musped his.  The more in therrud ikey theare tera him he corees Oof had had as the had that at Domlesbem had he, bant the had as for had beanred as the douplly thear on had and thet to hid upelime the been sif there housl thile in thit ult De his the would his pfot could solleche theing herd and se lill htowers, lamid thear as his the thear had, sit he weriry on woult he his mom of acus yee vloo haok and if sher for his his had been the ticas his seen sisieter him semored en that Harry been then the  onbon.\n",
      "\tIt sarpest hig been his canibed Dust amould bous, voomed hit bed the would ave he has earing allouss his Rilly had lane shain of mkit theted it surdive his was a woretort him the the wat had his Worevor so p\n"
     ]
    }
   ],
   "source": [
    "first_char = \" \"\n",
    "x_input = encode_string(first_char)\n",
    "Y_t, s_t = synthetize_seq(\n",
    "    LSTM, \n",
    "    torch.zeros((m, 1), dtype=torch.double), \n",
    "    torch.zeros((m, 1), dtype=torch.double),\n",
    "    x_input[:,0], 1000, 0.8)\n",
    "print(first_char + s_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, step, epoch = 0, 0, 0\n",
    "n_epochs = 10\n",
    "smooth_loss = 0\n",
    "seq_length = 25\n",
    "losses = []\n",
    "hprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "\n",
    "eta = 0.0005\n",
    "beta_1, beta_2, epsilon = 0.9, 0.999, 1e-8\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "vb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "vc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "vU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "vV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "vW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "vs = {'b': vb, 'c': vc, 'U': vU, 'V': vV, 'W': vW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    X_chars = book_data[e:e+seq_length]\n",
    "    Y_chars = book_data[e+1:e+seq_length+1]\n",
    "    X_train = encode_string(X_chars)\n",
    "    Y_train = encode_string(Y_chars)\n",
    "\n",
    "    A_train, H_train, P_train, ht = forward(RNN, X_train, hprev)\n",
    "    loss = compute_loss(Y_train, P_train)\n",
    "    grads, grads_clamped = backward(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "    for k in ms.keys():\n",
    "        ms[k] = beta_1*ms[k] + (1 - beta_1)*grads_clamped[k]\n",
    "        vs[k] = beta_2*vs[k] + (1 - beta_2)*(grads_clamped[k]**2)\n",
    "        m_hat = ms[k]/(1 - beta_1**(step+1))\n",
    "        v_hat = vs[k]/(1 - beta_2**(step+1))\n",
    "        RNN[k] -= (eta/torch.sqrt(v_hat + epsilon))*m_hat\n",
    "\n",
    "    if step == 0:\n",
    "        smooth_loss = loss\n",
    "    else:\n",
    "        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "\n",
    "    losses.append(smooth_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"\\t * Smooth loss: {smooth_loss:.4f}\")\n",
    "    if step % 5000 == 0:\n",
    "        _, s_syn = synthetize_seq(RNN, hprev, X_train[:, 0], 200)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "        print(\"-\" * 100)\n",
    "    if step % 100000 == 0 and step > 0:\n",
    "        _, s_lsyn = synthetize_seq(RNN, hprev, X_train[:, 0], 1000)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    step += 1\n",
    "    e += seq_length\n",
    "    if e > len(book_data) - seq_length:\n",
    "        e = 0\n",
    "        epoch += 1\n",
    "        hprev = torch.zeros((m, 1), dtype=torch.double)\n",
    "    else:\n",
    "        hprev = ht\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_batch(rnn, X, hprev):\n",
    "    K, seq_length, batch_size = X.shape\n",
    "    m = hprev.shape[0]  # (m, batch_size)\n",
    "\n",
    "    P = torch.zeros((K, seq_length, batch_size), dtype=torch.double)\n",
    "    A = torch.zeros((m, seq_length, batch_size), dtype=torch.double)\n",
    "    H = torch.zeros((m, seq_length, batch_size), dtype=torch.double)\n",
    "\n",
    "    ht = hprev.clone()\n",
    "    for i in range(seq_length):\n",
    "        xt = X[:, i, :]  # Access the ith timestep across all batches\n",
    "        at = torch.mm(rnn['W'], ht) + torch.mm(rnn['U'], xt) + rnn['b'].expand(m, batch_size)\n",
    "        ht = torch.tanh(at)\n",
    "        ot = torch.mm(rnn['V'], ht) + rnn['c'].expand(K, batch_size)\n",
    "        pt = F.softmax(ot, dim=0)\n",
    "\n",
    "        H[:, i, :] = ht\n",
    "        P[:, i, :] = pt\n",
    "        A[:, i, :] = at\n",
    "\n",
    "    return A, H, P, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_batch(Y, P):\n",
    "    batch_size = Y.shape[2]\n",
    "    log_probs = torch.log(P)\n",
    "    cross_entropy = -torch.sum(Y * log_probs)\n",
    "    loss = cross_entropy.item() / batch_size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_batch(rnn, X, Y, A, H, P, hprev):\n",
    "    dA = torch.zeros_like(A)\n",
    "    dH = torch.zeros_like(H)\n",
    "\n",
    "    G = -(Y - P)\n",
    "    dV = torch.bmm(G.permute(2, 0, 1), H.permute(2, 1, 0)).mean(dim=0)\n",
    "    dhtau = torch.matmul(G[:, -1, :].t(), rnn['V']).t()\n",
    "    datau = (1 - torch.pow(torch.tanh(A[:, -1, :]), 2)) * dhtau\n",
    "    dH[:, -1, :] = dhtau\n",
    "    dA[:, -1, :] = datau\n",
    "\n",
    "    for i in range(seq_length - 2, -1, -1):\n",
    "        dht = torch.matmul(G[:, i, :].t(), rnn['V']).t() + torch.matmul(dA[:, i+1, :].t(), rnn['W']).t()\n",
    "        dat = (1 - torch.pow(torch.tanh(A[:, i]), 2)) * dht\n",
    "        dH[:, i] = dht\n",
    "        dA[:, i] = dat\n",
    "\n",
    "    Hd = torch.cat((hprev.reshape((m, 1, -1)), H[:, :-1, :]), dim=1)\n",
    "    dW = torch.matmul(dA.permute(2, 0, 1), Hd.permute(2, 1, 0)).mean(dim=0)\n",
    "    dU = torch.matmul(dA.permute(2, 0, 1), X.permute(2, 1, 0)).mean(dim=0)\n",
    "    dc = G.sum(1).mean(dim=1).reshape((-1, 1))\n",
    "    db = dA.sum(1).mean(dim=1).reshape((-1, 1))\n",
    "    grads = {'U': dU, 'W': dW, 'V': dV, 'c': dc, 'b': db}\n",
    "    grads_clamped = {k: torch.clamp(v, min=-5.0, max=5.0) for (k,v) in grads.items()}\n",
    "    return grads, grads_clamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, step, epoch = 0, 0, 0\n",
    "n_epochs = 500\n",
    "smooth_loss = 0\n",
    "batch_size = 32\n",
    "seq_length = 150\n",
    "eta = 0.1\n",
    "losses = []\n",
    "hprev = torch.zeros((m, batch_size), dtype=torch.double)\n",
    "\n",
    "mb = torch.zeros_like(RNN['b'], dtype=torch.float)\n",
    "mc = torch.zeros_like(RNN['c'], dtype=torch.float)\n",
    "mU = torch.zeros_like(RNN['U'], dtype=torch.float)\n",
    "mV = torch.zeros_like(RNN['V'], dtype=torch.float)\n",
    "mW = torch.zeros_like(RNN['W'], dtype=torch.float)\n",
    "ms = {'b': mb, 'c': mc, 'U': mU, 'V': mV, 'W': mW}\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    X_batch = []\n",
    "    Y_batch = []\n",
    "    for b in range(batch_size):\n",
    "        start_index = e + b * seq_length\n",
    "        X_chars = book_data[start_index:(start_index + seq_length)]\n",
    "        Y_chars = book_data[(start_index + 1):(start_index + seq_length + 1)]\n",
    "        X_batch.append(encode_string(X_chars))\n",
    "        Y_batch.append(encode_string(Y_chars))\n",
    "\n",
    "    X_train = torch.stack(X_batch, dim=2)  # shape: (K, seq_length, n_batch)\n",
    "    Y_train = torch.stack(Y_batch, dim=2)  # shape: (K, seq_length, n_batch)\n",
    "\n",
    "    A_train, H_train, P_train, hts = forward_batch(RNN, X_train, hprev)\n",
    "    loss = compute_loss_batch(Y_train, P_train)\n",
    "    grads, grads_clamped = backward_batch(RNN, X_train, Y_train, A_train, H_train, P_train, hprev)\n",
    "\n",
    "    for k in ms.keys():\n",
    "        ms[k] += grads_clamped[k]**2\n",
    "        RNN[k] -= (eta/torch.sqrt(ms[k] + epsilon)) * grads_clamped[k]\n",
    "\n",
    "    if step == 0:\n",
    "        smooth_loss = loss\n",
    "    else:\n",
    "        smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "    losses.append(smooth_loss)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step: {step}\")\n",
    "        print(f\"\\t * Smooth loss: {smooth_loss:.4f}\")\n",
    "    if step % 5000 == 0:\n",
    "        _, s_syn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 200, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Synthetized sequence: \\n{s_syn}\")\n",
    "        print(\"-\" * 100)\n",
    "    if step % 100000 == 0 and step > 0:\n",
    "        _, s_lsyn = synthetize_seq(RNN, hprev[:, 0:1], X_train[:, 0, 0], 1000, 0.6)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Long synthetized sequence: \\n{s_lsyn}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    step += 1\n",
    "    e += batch_size * seq_length\n",
    "    if e > len(book_data) - batch_size * seq_length:\n",
    "        e = 0\n",
    "        epoch += 1\n",
    "        hprev = torch.zeros((m, batch_size), dtype=torch.double)\n",
    "    else:\n",
    "        hprev = hts\n",
    "\n",
    "with open(f'rnn_{time.time()}.pickle', 'wb') as handle:\n",
    "    pickle.dump(RNN, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
